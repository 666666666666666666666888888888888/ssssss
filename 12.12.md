1.基本环境准备
	1.1指定流处理环境
	 1.2设置并行度
	 env.setParallelism(1);

2.检查点的相关设置
	 2.1开启检查点
	 env.enableCheckpointing(5000);       env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
	 2.2设点检查点超时时间
	 env.getCheckpointConfig().setCheckpointTimeout(10000);
	 2.3设置job取消检查点是否保留env.getCheckpointConfig().setExternalizedCheckpointCleanup(RETAIN_ON_CANCELLATION);
	 2.4设置两个检查点之间最小时间间隔
	 env.getCheckpointConfig().setMinPauseBetweenCheckpoints(5000);
	 2.5设置重启策略
	 env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,3000));
	 env.setRestartStrategy(RestartStrategies.failureRateRestart(3, Time.days(30),Time.seconds(3)));
	 2.6设置状态后端以及检查点存储路径env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop102:8020/gmall2023/stream/"+groupId);
	 状态后端:
	 env.setStateBackend(new HashMapStateBackend());
	 2.7设置操作hadoop的用户
	 
3.从kafka的topic_db主题中读取业务数据
	String topic="topic_db"
	String groupId="dim_app_group"
	
	 3.1声明消费的主题以及消费者组
	 3.2创建消费者对象
	 3.3消费数据 封装为流
	 
4.对业务流中数据类型进行转换  jsonString->jsonObject

5.使用FlinkCDC读取配置表中的配置信息
	 5.1创建MySQLSource对象
	 5.2读取数据 封装为流

6.对配置流中的数据类型进行转换 jsonString->jsonObject

7.根据配置表中的信息到Habase中执行建表或者删除表操作

8.将配置流中的配置信息进行广播 --broadcast

9.将主流业务数据和广播流配置信息将进行关联 --connect

10.处理关联后的数据(判断是否为维度)
	   10.1processElement :处理主流业务数据                          根据维度表名到广播状态中读取配置信息,判断是否为维度
	   10.2 processBroadcastElement:处理广播流配置信息       将配置数据放到广播状态中   k:维度表名  v:一个配置表
	   
11.将维度表同步到Hbase表中
